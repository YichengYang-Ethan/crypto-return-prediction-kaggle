{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "5a0c0017",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-14T01:26:56.231759Z",
     "iopub.status.busy": "2025-08-14T01:26:56.231486Z",
     "iopub.status.idle": "2025-08-14T02:00:09.575633Z",
     "shell.execute_reply": "2025-08-14T02:00:09.574608Z"
    },
    "papermill": {
     "duration": 1993.375003,
     "end_time": "2025-08-14T02:00:09.603615",
     "exception": false,
     "start_time": "2025-08-14T01:26:56.228612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Cryptocurrency 24-Hour Return Prediction\n\nLightGBM pipeline for predicting 24-hour forward returns across 355 cryptocurrencies, built for the **Avenir HKU Web3 Quant Competition**.\n\n**Methodology**: Feature engineering (RSI, MACD, Bollinger Bands, ATR, OBV, rolling stats, lagged returns) followed by LightGBM with TimeSeriesSplit cross-validation (5 folds, train on past, validate on future)."
  },
  {
   "cell_type": "markdown",
   "id": "26fhlhp1yp5",
   "source": "## Setup and Configuration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pdp38hp6ued",
   "source": "import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm\nimport warnings\nimport gc\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error\n\nwarnings.filterwarnings('ignore')\n\n# --- Configuration ---\nTRAIN_DATA_PATH = \"/kaggle/input/avenir-hku-web/kline_data/train_data\"\nSUBMISSION_ID_PATH = \"/kaggle/input/avenir-hku-web/submission_id.csv\"\nOUTPUT_PATH = \"/kaggle/working/submission.csv\"\nN_SPLITS = 5",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4690fzo52nw",
   "source": "## Data Loading",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2yul3qpr20e",
   "source": "def load_all_crypto_data(data_path):\n    all_files = [f for f in os.listdir(data_path) if f.endswith('.parquet')]\n    df_list = []\n    print(f\"Loading {len(all_files)} files...\")\n    for file in tqdm(all_files):\n        symbol = file.split('.')[0]\n        file_path = os.path.join(data_path, file)\n        try:\n            df = pd.read_parquet(file_path)\n            df['symbol'] = symbol\n            df_list.append(df)\n        except Exception as e:\n            print(f\"Could not read file {file}: {e}\")\n    if not df_list: return pd.DataFrame()\n    full_df = pd.concat(df_list, ignore_index=True)\n    full_df['timestamp'] = pd.to_datetime(full_df['timestamp'], unit='ms')\n    full_df = full_df.sort_values(by=['symbol', 'timestamp']).reset_index(drop=True)\n    return full_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vpp2fvdzio",
   "source": "## Feature Engineering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4z8sxial9u3",
   "source": "def feature_engineering_for_symbol(df):\n    data = df.copy().sort_values('timestamp')\n    data['rsi_14'] = calculate_rsi(data, 14)\n    data['macd'], data['macd_signal'] = calculate_macd(data)\n    data['roc_12'] = (data['close'].diff(12) / data['close'].shift(12)) * 100\n    data['upper_band'], data['lower_band'] = calculate_bollinger_bands(data, 20)\n    data['atr_14'] = calculate_atr(data, 14)\n    data['bb_width'] = (data['upper_band'] - data['lower_band']) / data['close'].rolling(20).mean()\n    data['obv'] = calculate_obv(data)\n    for w in [10, 30, 60]:\n        data[f'rolling_mean_{w}'] = data['close'].rolling(window=w).mean()\n        data[f'rolling_std_{w}'] = data['close'].rolling(window=w).std()\n        data[f'rolling_vol_mean_{w}'] = data['volume'].rolling(window=w).mean()\n    data['return'] = data['close'].pct_change()\n    for lag in [1, 2, 3, 4, 5]:\n        data[f'return_lag_{lag}'] = data['return'].shift(lag)\n    return data\n\ndef calculate_rsi(data, period=14):\n    \"\"\"RSI using Wilder smoothing (EWM with alpha=1/period).\"\"\"\n    delta = data['close'].diff(1)\n    gain = delta.where(delta > 0, 0)\n    loss = -delta.where(delta < 0, 0)\n    avg_gain = gain.ewm(alpha=1/period, adjust=False).mean()\n    avg_loss = loss.ewm(alpha=1/period, adjust=False).mean()\n    rs = avg_gain / avg_loss\n    return 100 - (100 / (1 + rs))\n\ndef calculate_macd(data, fast_period=12, slow_period=26, signal_period=9):\n    ema_fast = data['close'].ewm(span=fast_period, adjust=False).mean()\n    ema_slow = data['close'].ewm(span=slow_period, adjust=False).mean()\n    macd_line = ema_fast - ema_slow\n    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()\n    return macd_line, signal_line\n\ndef calculate_bollinger_bands(data, window=20, std_dev=2):\n    sma = data['close'].rolling(window=window).mean()\n    std = data['close'].rolling(window=window).std()\n    upper_band = sma + (std * std_dev)\n    lower_band = sma - (std * std_dev)\n    return upper_band, lower_band\n\ndef calculate_atr(data, window=14):\n    high_low = data['high'] - data['low']\n    high_prev_close = np.abs(data['high'] - data['close'].shift(1))\n    low_prev_close = np.abs(data['low'] - data['close'].shift(1))\n    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n    return tr.ewm(span=window, adjust=False).mean()\n\ndef calculate_obv(data):\n    obv = (np.sign(data['close'].diff()) * data['volume']).fillna(0).cumsum()\n    return obv",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "niata91vbgp",
   "source": "## Training with TimeSeriesSplit Cross-Validation\n\nTrain a LightGBM ensemble using 5-fold time-series cross-validation. Each fold trains on past data and validates on the subsequent temporal block, preventing data leakage. The final model averages predictions across all 5 fold models.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9s3wtpi663a",
   "source": "print(\"STAGE 1: TRAINING\")\ndf_full = load_all_crypto_data(TRAIN_DATA_PATH)\nif not df_full.empty:\n    rename_dict = {'open_price': 'open', 'high_price': 'high', 'low_price': 'low', 'close_price': 'close'}\n    df_full.rename(columns=rename_dict, inplace=True)\n    numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount', 'count', 'buy_volume', 'buy_amount']\n    for col in numeric_cols:\n        if col in df_full.columns:\n            df_full[col] = pd.to_numeric(df_full[col], errors='coerce').astype('float32')\n    df_features = df_full.groupby('symbol', as_index=False).apply(feature_engineering_for_symbol).reset_index(drop=True)\n    del df_full; gc.collect()\n    future_periods = 96\n    df_features['target'] = df_features.groupby('symbol')['close'].shift(-future_periods) / df_features['close'] - 1\n    df_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df_train = df_features.dropna(subset=['target'])\n    features = [col for col in df_train.columns if col not in ['timestamp', 'symbol', 'target', 'open', 'high', 'low', 'close', 'volume', 'amount']]\n\n    # Sort by timestamp to ensure temporal ordering for TimeSeriesSplit\n    df_train = df_train.sort_values('timestamp').reset_index(drop=True)\n    X = df_train[features]\n    y = df_train['target']\n\n    # TimeSeriesSplit: always train on past, validate on future\n    tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n    fold_models = []\n    fold_scores = []\n    last_fold_val = {}  # Store last fold's validation data for analysis\n\n    print(f\"Training with {N_SPLITS}-fold TimeSeriesSplit...\")\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        lgbm = lgb.LGBMRegressor(\n            objective='regression_l1', n_estimators=1000, learning_rate=0.05,\n            num_leaves=31, max_depth=8, subsample=0.8, colsample_bytree=0.8,\n            random_state=42, n_jobs=-1\n        )\n        lgbm.fit(X_train, y_train,\n                 eval_set=[(X_val, y_val)], eval_metric='l1',\n                 callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n\n        val_pred = lgbm.predict(X_val)\n        mae = mean_absolute_error(y_val, val_pred)\n        fold_scores.append(mae)\n        fold_models.append(lgbm)\n        print(f\"  Fold {fold + 1}/{N_SPLITS} â€” Validation MAE: {mae:.6f}\")\n\n        # Save last fold validation data for baseline comparison and L/S simulation\n        last_fold_val = {\n            'X_val': X_val, 'y_val': y_val, 'y_pred': val_pred,\n            'timestamps': df_train.iloc[val_idx]['timestamp'].values,\n            'symbols': df_train.iloc[val_idx]['symbol'].values,\n        }\n\n    mean_mae = np.mean(fold_scores)\n    std_mae = np.std(fold_scores)\n    print(f\"\\nCV Results: MAE = {mean_mae:.6f} +/- {std_mae:.6f}\")\n    print(\"Model training completed successfully!\")\n    del X_train, y_train, X_val, y_val; gc.collect()\nelse:\n    print(\"Execution stopped because no data was loaded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gcm16s4j72k",
   "source": "## SHAP Feature Importance\n\nUse TreeExplainer to identify which features drive the model's predictions. This provides interpretable, per-feature contribution analysis beyond simple split-based importance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jncwnmyfwg",
   "source": "import shap\n\n# Use the last fold model and a sample of validation data\nmodel = fold_models[-1]\nX_test_sample = last_fold_val['X_val'].sample(n=min(5000, len(last_fold_val['X_val'])), random_state=42)\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test_sample)\n\nshap.summary_plot(shap_values, X_test_sample, max_display=15, show=False)\nplt.tight_layout()\nplt.savefig(\"shap_importance.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n\n# Print top features by mean absolute SHAP value\nshap_importance = pd.DataFrame({\n    'feature': X_test_sample.columns,\n    'mean_abs_shap': np.abs(shap_values).mean(axis=0)\n}).sort_values('mean_abs_shap', ascending=False)\nprint(\"Top 10 features by mean |SHAP|:\")\nprint(shap_importance.head(10).to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "j58k47n56b9",
   "source": "## Naive Baseline Comparison\n\nCompare the model's MAE against two simple baselines to quantify how much value the model adds:\n- **Zero baseline**: Always predict 0 return (MAE = mean of |actual returns|)\n- **Persistence baseline**: Predict the last known return (most recent lagged return)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "e831inz15so",
   "source": "y_actual = last_fold_val['y_val']\ny_pred = last_fold_val['y_pred']\n\n# Model MAE (last fold)\nmodel_mae = mean_absolute_error(y_actual, y_pred)\n\n# Zero baseline: predict 0 for all returns\nzero_mae = np.mean(np.abs(y_actual))\n\n# Persistence baseline: predict last known return (return_lag_1 feature)\npersistence_pred = last_fold_val['X_val']['return_lag_1'].fillna(0).values\npersistence_mae = mean_absolute_error(y_actual, persistence_pred)\n\n# Improvement percentages\nzero_improvement = (zero_mae - model_mae) / zero_mae * 100\npersistence_improvement = (persistence_mae - model_mae) / persistence_mae * 100\n\nprint(\"=\" * 55)\nprint(\"BASELINE COMPARISON (Last Fold Validation Set)\")\nprint(\"=\" * 55)\nprint(f\"  Model MAE:        {model_mae:.6f}\")\nprint(f\"  Zero Baseline:    {zero_mae:.6f}  (predict 0)\")\nprint(f\"  Persistence:      {persistence_mae:.6f}  (predict last return)\")\nprint(\"-\" * 55)\nprint(f\"  vs Zero:          {zero_improvement:+.2f}% improvement\")\nprint(f\"  vs Persistence:   {persistence_improvement:+.2f}% improvement\")\nprint(\"=\" * 55)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "p3pn6bk9kc",
   "source": "## Long/Short Strategy Simulation\n\nTest whether the model's predictions have **economic value** beyond statistical accuracy. For each time step in the validation set:\n- Rank all 355 assets by predicted return\n- Go **long** the top decile, **short** the bottom decile (equal weight)\n- Compute portfolio returns, cumulative performance, Sharpe ratio, and max drawdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5kzfbd6fsec",
   "source": "# Build a DataFrame with predictions and actuals per timestamp/symbol\nls_df = pd.DataFrame({\n    'timestamp': last_fold_val['timestamps'],\n    'symbol': last_fold_val['symbols'],\n    'y_actual': last_fold_val['y_val'].values,\n    'y_pred': last_fold_val['y_pred'],\n})\n\n# For each timestamp, rank predictions and compute L/S return\nls_returns = []\nfor ts, group in ls_df.groupby('timestamp'):\n    if len(group) < 10:\n        continue\n    n_decile = max(1, len(group) // 10)\n    ranked = group.sort_values('y_pred')\n    short_leg = ranked.head(n_decile)['y_actual'].mean()\n    long_leg = ranked.tail(n_decile)['y_actual'].mean()\n    ls_return = long_leg - short_leg  # L/S spread\n    ls_returns.append({'timestamp': ts, 'ls_return': ls_return})\n\nls_series = pd.DataFrame(ls_returns).sort_values('timestamp').reset_index(drop=True)\nls_series['cumulative'] = (1 + ls_series['ls_return']).cumprod()\n\n# Performance metrics\ntotal_return = ls_series['cumulative'].iloc[-1] - 1\nn_periods = len(ls_series)\n# Annualize assuming 15-min bars: 96 bars/day * 365 days\nperiods_per_year = 96 * 365\nannualized_return = (1 + total_return) ** (periods_per_year / max(n_periods, 1)) - 1\nannualized_vol = ls_series['ls_return'].std() * np.sqrt(periods_per_year)\nsharpe = annualized_return / annualized_vol if annualized_vol > 0 else 0\nrunning_max = ls_series['cumulative'].cummax()\ndrawdown = (ls_series['cumulative'] - running_max) / running_max\nmax_drawdown = drawdown.min()\n\nprint(\"=\" * 55)\nprint(\"LONG/SHORT STRATEGY (Top vs Bottom Decile)\")\nprint(\"=\" * 55)\nprint(f\"  Periods:             {n_periods}\")\nprint(f\"  Cumulative Return:   {total_return:.4%}\")\nprint(f\"  Annualized Sharpe:   {sharpe:.2f}\")\nprint(f\"  Max Drawdown:        {max_drawdown:.4%}\")\nprint(\"=\" * 55)\n\n# Plot cumulative return\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(ls_series['timestamp'], ls_series['cumulative'], linewidth=1.2)\nax.set_title('Long/Short Portfolio: Cumulative Return (Top vs Bottom Decile)')\nax.set_xlabel('Time')\nax.set_ylabel('Cumulative Return (1 = start)')\nax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(\"ls_cumulative_return.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n\ndel X, y, df_train, df_features; gc.collect()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i9ptdlqc8hh",
   "source": "## Prediction and Submission\n\nGenerate predictions by averaging across all 5 fold models. Process symbols one at a time to stay within Kaggle memory limits.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "63qdlr9sg98",
   "source": "if fold_models:\n    print(\"STAGE 2: PREDICTION (Memory-Safe Symbol by Symbol)\")\n\n    print(\"Step 1 (Pred): Loading all data for prediction...\")\n    df_pred_base = load_all_crypto_data(TRAIN_DATA_PATH)\n    df_pred_base.rename(columns=rename_dict, inplace=True)\n    for col in numeric_cols:\n        if col in df_pred_base.columns:\n            df_pred_base[col] = pd.to_numeric(df_pred_base[col], errors='coerce').astype('float32')\n\n    all_predictions = []\n    symbols = df_pred_base['symbol'].unique()\n\n    print(f\"Step 2 (Pred): Processing {len(symbols)} symbols one by one...\")\n    for symbol in tqdm(symbols):\n        df_symbol = df_pred_base[df_pred_base['symbol'] == symbol]\n        df_symbol_features = feature_engineering_for_symbol(df_symbol)\n        X_pred = df_symbol_features[features]\n        X_pred.fillna(0, inplace=True)\n        # Average predictions across all fold models\n        preds = np.mean([model.predict(X_pred) for model in fold_models], axis=0)\n        result_df = pd.DataFrame({\n            'timestamp': df_symbol_features['timestamp'],\n            'symbol': df_symbol_features['symbol'],\n            'predict_return': preds\n        })\n        all_predictions.append(result_df)\n\n    print(\"Optimizing memory before final concatenation...\")\n    del df_pred_base\n    gc.collect()\n\n    print(\"Step 3 (Pred): Concatenating all predictions...\")\n    final_pred_df = pd.concat(all_predictions, ignore_index=True)\n    del all_predictions\n    gc.collect()\n\n    print(\"Step 4 (Pred): Formatting submission file...\")\n    final_pred_df['timestamp'] = final_pred_df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    final_pred_df['id'] = final_pred_df['timestamp'] + \"_\" + final_pred_df['symbol']\n\n    df_submission_id = pd.read_csv(SUBMISSION_ID_PATH)\n    final_submission = pd.merge(df_submission_id, final_pred_df[['id', 'predict_return']], on='id', how='left')\n    final_submission['predict_return'].fillna(0, inplace=True)\n\n    final_submission.to_csv(OUTPUT_PATH, index=False)\n\n    print(f\"Submission file successfully generated at: {OUTPUT_PATH}\")\n    print(\"File preview:\")\n    print(final_submission.head())\n    print(f\"File shape: {final_submission.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12985119,
     "sourceId": 104935,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2000.288644,
   "end_time": "2025-08-14T02:00:12.652182",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-14T01:26:52.363538",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}