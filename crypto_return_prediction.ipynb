{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "5a0c0017",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-14T01:26:56.231759Z",
     "iopub.status.busy": "2025-08-14T01:26:56.231486Z",
     "iopub.status.idle": "2025-08-14T02:00:09.575633Z",
     "shell.execute_reply": "2025-08-14T02:00:09.574608Z"
    },
    "papermill": {
     "duration": 1993.375003,
     "end_time": "2025-08-14T02:00:09.603615",
     "exception": false,
     "start_time": "2025-08-14T01:26:56.228612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Cryptocurrency 24-Hour Return Prediction\n\nLightGBM pipeline for predicting 24-hour forward returns across 355 cryptocurrencies, built for the **Avenir HKU Web3 Quant Competition**.\n\n**Methodology**: Feature engineering (RSI, MACD, Bollinger Bands, ATR, OBV, rolling stats, lagged returns) followed by LightGBM with TimeSeriesSplit cross-validation (5 folds, train on past, validate on future)."
  },
  {
   "cell_type": "markdown",
   "id": "26fhlhp1yp5",
   "source": "## Setup and Configuration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pdp38hp6ued",
   "source": "import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport os\nfrom tqdm import tqdm\nimport warnings\nimport gc\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error\n\nwarnings.filterwarnings('ignore')\n\n# --- Configuration ---\nTRAIN_DATA_PATH = \"/kaggle/input/avenir-hku-web/kline_data/train_data\"\nSUBMISSION_ID_PATH = \"/kaggle/input/avenir-hku-web/submission_id.csv\"\nOUTPUT_PATH = \"/kaggle/working/submission.csv\"\nN_SPLITS = 5",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4690fzo52nw",
   "source": "## Data Loading",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2yul3qpr20e",
   "source": "def load_all_crypto_data(data_path):\n    all_files = [f for f in os.listdir(data_path) if f.endswith('.parquet')]\n    df_list = []\n    print(f\"Loading {len(all_files)} files...\")\n    for file in tqdm(all_files):\n        symbol = file.split('.')[0]\n        file_path = os.path.join(data_path, file)\n        try:\n            df = pd.read_parquet(file_path)\n            df['symbol'] = symbol\n            df_list.append(df)\n        except Exception as e:\n            print(f\"Could not read file {file}: {e}\")\n    if not df_list: return pd.DataFrame()\n    full_df = pd.concat(df_list, ignore_index=True)\n    full_df['timestamp'] = pd.to_datetime(full_df['timestamp'], unit='ms')\n    full_df = full_df.sort_values(by=['symbol', 'timestamp']).reset_index(drop=True)\n    return full_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vpp2fvdzio",
   "source": "## Feature Engineering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4z8sxial9u3",
   "source": "def feature_engineering_for_symbol(df):\n    data = df.copy().sort_values('timestamp')\n    data['rsi_14'] = calculate_rsi(data, 14)\n    data['macd'], data['macd_signal'] = calculate_macd(data)\n    data['roc_12'] = (data['close'].diff(12) / data['close'].shift(12)) * 100\n    data['upper_band'], data['lower_band'] = calculate_bollinger_bands(data, 20)\n    data['atr_14'] = calculate_atr(data, 14)\n    data['bb_width'] = (data['upper_band'] - data['lower_band']) / data['close'].rolling(20).mean()\n    data['obv'] = calculate_obv(data)\n    for w in [10, 30, 60]:\n        data[f'rolling_mean_{w}'] = data['close'].rolling(window=w).mean()\n        data[f'rolling_std_{w}'] = data['close'].rolling(window=w).std()\n        data[f'rolling_vol_mean_{w}'] = data['volume'].rolling(window=w).mean()\n    data['return'] = data['close'].pct_change()\n    for lag in [1, 2, 3, 4, 5]:\n        data[f'return_lag_{lag}'] = data['return'].shift(lag)\n    return data\n\ndef calculate_rsi(data, window=14):\n    delta = data['close'].diff(1)\n    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n    rs = gain / loss\n    return 100 - (100 / (1 + rs))\n\ndef calculate_macd(data, fast_period=12, slow_period=26, signal_period=9):\n    ema_fast = data['close'].ewm(span=fast_period, adjust=False).mean()\n    ema_slow = data['close'].ewm(span=slow_period, adjust=False).mean()\n    macd_line = ema_fast - ema_slow\n    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()\n    return macd_line, signal_line\n\ndef calculate_bollinger_bands(data, window=20, std_dev=2):\n    sma = data['close'].rolling(window=window).mean()\n    std = data['close'].rolling(window=window).std()\n    upper_band = sma + (std * std_dev)\n    lower_band = sma - (std * std_dev)\n    return upper_band, lower_band\n\ndef calculate_atr(data, window=14):\n    high_low = data['high'] - data['low']\n    high_prev_close = np.abs(data['high'] - data['close'].shift(1))\n    low_prev_close = np.abs(data['low'] - data['close'].shift(1))\n    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n    return tr.ewm(span=window, adjust=False).mean()\n\ndef calculate_obv(data):\n    obv = (np.sign(data['close'].diff()) * data['volume']).fillna(0).cumsum()\n    return obv",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "niata91vbgp",
   "source": "## Training with TimeSeriesSplit Cross-Validation\n\nTrain a LightGBM ensemble using 5-fold time-series cross-validation. Each fold trains on past data and validates on the subsequent temporal block, preventing data leakage. The final model averages predictions across all 5 fold models.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9s3wtpi663a",
   "source": "print(\"STAGE 1: TRAINING\")\ndf_full = load_all_crypto_data(TRAIN_DATA_PATH)\nif not df_full.empty:\n    rename_dict = {'open_price': 'open', 'high_price': 'high', 'low_price': 'low', 'close_price': 'close'}\n    df_full.rename(columns=rename_dict, inplace=True)\n    numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount', 'count', 'buy_volume', 'buy_amount']\n    for col in numeric_cols:\n        if col in df_full.columns:\n            df_full[col] = pd.to_numeric(df_full[col], errors='coerce').astype('float32')\n    df_features = df_full.groupby('symbol', as_index=False).apply(feature_engineering_for_symbol).reset_index(drop=True)\n    del df_full; gc.collect()\n    future_periods = 96\n    df_features['target'] = df_features.groupby('symbol')['close'].shift(-future_periods) / df_features['close'] - 1\n    df_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df_train = df_features.dropna(subset=['target'])\n    features = [col for col in df_train.columns if col not in ['timestamp', 'symbol', 'target', 'open', 'high', 'low', 'close', 'volume', 'amount']]\n\n    # Sort by timestamp to ensure temporal ordering for TimeSeriesSplit\n    df_train = df_train.sort_values('timestamp').reset_index(drop=True)\n    X = df_train[features]\n    y = df_train['target']\n    del df_train, df_features; gc.collect()\n\n    # TimeSeriesSplit: always train on past, validate on future\n    tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n    fold_models = []\n    fold_scores = []\n\n    print(f\"Training with {N_SPLITS}-fold TimeSeriesSplit...\")\n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        lgbm = lgb.LGBMRegressor(\n            objective='regression_l1', n_estimators=1000, learning_rate=0.05,\n            num_leaves=31, max_depth=8, subsample=0.8, colsample_bytree=0.8,\n            random_state=42, n_jobs=-1\n        )\n        lgbm.fit(X_train, y_train,\n                 eval_set=[(X_val, y_val)], eval_metric='l1',\n                 callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n\n        val_pred = lgbm.predict(X_val)\n        mae = mean_absolute_error(y_val, val_pred)\n        fold_scores.append(mae)\n        fold_models.append(lgbm)\n        print(f\"  Fold {fold + 1}/{N_SPLITS} â€” Validation MAE: {mae:.6f}\")\n\n    mean_mae = np.mean(fold_scores)\n    std_mae = np.std(fold_scores)\n    print(f\"\\nCV Results: MAE = {mean_mae:.6f} +/- {std_mae:.6f}\")\n    print(\"Model training completed successfully!\")\n    del X, y; gc.collect()\nelse:\n    print(\"Execution stopped because no data was loaded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i9ptdlqc8hh",
   "source": "## Prediction and Submission\n\nGenerate predictions by averaging across all 5 fold models. Process symbols one at a time to stay within Kaggle memory limits.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "63qdlr9sg98",
   "source": "if fold_models:\n    print(\"STAGE 2: PREDICTION (Memory-Safe Symbol by Symbol)\")\n\n    print(\"Step 1 (Pred): Loading all data for prediction...\")\n    df_pred_base = load_all_crypto_data(TRAIN_DATA_PATH)\n    df_pred_base.rename(columns=rename_dict, inplace=True)\n    for col in numeric_cols:\n        if col in df_pred_base.columns:\n            df_pred_base[col] = pd.to_numeric(df_pred_base[col], errors='coerce').astype('float32')\n\n    all_predictions = []\n    symbols = df_pred_base['symbol'].unique()\n\n    print(f\"Step 2 (Pred): Processing {len(symbols)} symbols one by one...\")\n    for symbol in tqdm(symbols):\n        df_symbol = df_pred_base[df_pred_base['symbol'] == symbol]\n        df_symbol_features = feature_engineering_for_symbol(df_symbol)\n        X_pred = df_symbol_features[features]\n        X_pred.fillna(0, inplace=True)\n        # Average predictions across all fold models\n        preds = np.mean([model.predict(X_pred) for model in fold_models], axis=0)\n        result_df = pd.DataFrame({\n            'timestamp': df_symbol_features['timestamp'],\n            'symbol': df_symbol_features['symbol'],\n            'predict_return': preds\n        })\n        all_predictions.append(result_df)\n\n    print(\"Optimizing memory before final concatenation...\")\n    del df_pred_base\n    gc.collect()\n\n    print(\"Step 3 (Pred): Concatenating all predictions...\")\n    final_pred_df = pd.concat(all_predictions, ignore_index=True)\n    del all_predictions\n    gc.collect()\n\n    print(\"Step 4 (Pred): Formatting submission file...\")\n    final_pred_df['timestamp'] = final_pred_df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n    final_pred_df['id'] = final_pred_df['timestamp'] + \"_\" + final_pred_df['symbol']\n\n    df_submission_id = pd.read_csv(SUBMISSION_ID_PATH)\n    final_submission = pd.merge(df_submission_id, final_pred_df[['id', 'predict_return']], on='id', how='left')\n    final_submission['predict_return'].fillna(0, inplace=True)\n\n    final_submission.to_csv(OUTPUT_PATH, index=False)\n\n    print(f\"Submission file successfully generated at: {OUTPUT_PATH}\")\n    print(\"File preview:\")\n    print(final_submission.head())\n    print(f\"File shape: {final_submission.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12985119,
     "sourceId": 104935,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2000.288644,
   "end_time": "2025-08-14T02:00:12.652182",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-14T01:26:52.363538",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}